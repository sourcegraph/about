---
title: "Episode 13: Andrew Gallant, creator of ripgrep"
publishDate: 2020-11-02T10:00-07:00
tags: [podcast]
slug: andrew-gallant
published: false
---

<!-- START AUDIO -->
<audio className="object-center" src="https://www.buzzsprout.com/1097978/6078358-andrew-gallant-creator-of-ripgrep.mp3" controls={true} preload="none"></audio>
<!-- END AUDIO -->

<!-- START GUESTS -->
<span>Andrew Gallant, Beyang Liu</span>
<!-- END GUESTS -->

<!-- START SUMMARY -->
[Andrew Gallant](https://twitter.com/burntsushi5) is the creator of [ripgrep](https://github.com/BurntSushi/ripgrep), a popular command-line search tool that powers the search box in VS Code. Andrew tells me how ripgrep began, explains why it's faster than GNU grep and other grep alternatives, and gets into the nitty-gritty of regex optimization.

We also discuss another matter near and dear to both of us: Linux window management. Andrew talks about what he likes about Go and Haskell and why Rust is his current go-to programming language, and finally he shares a humorous anecdote involving algorithms, technical recruiting, and everyone's favorite New England sports team.
<!-- END SUMMARY -->

<!-- START SHOWNOTES -->
Andrew Gallant: https://burntsushi.net, https://twitter.com/burntsushi5

ripgrep: https://github.com/BurntSushi/ripgrep

Bulletin board systems: vBulletin (https://en.wikipedia.org/wiki/VBulletin), YABB (http://www.yabbforum.com)

Window managers: [xmonad](https://xmonad.org/), [i3](https://i3wm.org/)

Russ Cox posts on implementing regular expressions: https://swtch.com/~rsc/regexp/

EWMH: https://en.wikipedia.org/wiki/Extended_Window_Manager_Hints

ICCCM: https://en.wikipedia.org/wiki/Inter-Client_Communication_Conventions_Manual

Xinerama: https://en.wikipedia.org/wiki/Xinerama

RandR: https://www.x.org/wiki/Projects/XRandR/

Openbox: http://openbox.org/wiki/Main_Page

wingo: https://github.com/BurntSushi/wingo

History of grep: https://en.wikipedia.org/wiki/Grep

GNU grep: https://www.gnu.org/software/grep/

Ken Thompson, author of original grep: https://en.wikipedia.org/wiki/Ken_Thompson

Silver searcher (`ag`): https://github.com/ggreer/the_silver_searcher

ack: https://github.com/beyondgrep/ack3

Ridiculous Fish post on Treacherous Optimization: https://ridiculousfish.com/blog/posts/old-age-and-treachery.html

Mike Haertel "why GNU grep is fast": https://lists.freebsd.org/pipermail/freebsd-current/2010-August/019310.html

GNU Parallel: https://www.gnu.org/software/parallel

xargs: https://en.wikipedia.org/wiki/Xargs

Boyer-Moore algorithm: https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string-search_algorithm

memchr: https://man7.org/linux/man-pages/man3/memchr.3.html

Rust memchr crate: https://crates.io/crates/memchr

Rust regex crate: https://github.com/rust-lang/regex

Vector instructions: https://en.wikipedia.org/wiki/Vector_processor#Vector_instructions

ripgrep FAQ: https://github.com/BurntSushi/ripgrep/blob/master/FAQ.md

Memory-mapped I/O: https://en.wikipedia.org/wiki/Memory-mapped_I/O

ripgrep in Visual Studio Code: https://github.com/microsoft/vscode-ripgrep, https://code.visualstudio.com/updates/v1_11#_text-search-improvements

PCRE2: https://www.pcre.org/current/doc/html/index.html

re2c: https://re2c.org

Backtracking regex implementation: https://users.cs.cf.ac.uk/Dave.Marshall/PERL/node80.html, https://docs.microsoft.com/en-us/dotnet/standard/base-types/backtracking-in-regular-expressions

Finite automaton regex implementation: https://deniskyashif.com/2019/02/17/implementing-a-regular-expression-engine/

Lazy DFA: https://github.com/rust-lang/regex/pull/164

Rust: https://www.rust-lang.org/

Norman Ramsey: https://engineering.tufts.edu/people/faculty/norman-ramsey

QuickCheck (Haskell): https://hackage.haskell.org/package/QuickCheck

Standard ML: https://en.wikipedia.org/wiki/Standard_ML

Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm

Andrew's blogpost "My FOSS Story": https://blog.burntsushi.net/foss

Bill Belichick: https://en.wikipedia.org/wiki/Bill_Belichick
<!-- END SHOWNOTES -->

<!-- START TRANSCRIPT -->
*This transcript was generated using auto-transcription software.*

Beyang: all right. I'm here with Andrew gland, creator of the popular command line search tool, rip grep, and prominent contributor in the go and rust opensource communities. Hey Andrew, how's it going?  Uh, not too bad, 

not too bad. uh, thanks for coming on the show today.

Andrew: No problem. Happy to be here.

Beyang: So there's a lot of cool content I'm excited to delve into. Um, but before we get into, you know, rip rap and, uh, all your opensource contributions, I always like to start off the conversation by asking people, what was your  inception point into the world of programming. How did you get started?

what was your journey from there?

Andrew: Yeah. Um, well, I'll try to give you the short story, but basically back in, I don't know, let's say middle school, early high school. Um, I was obsessed with the, uh, Nintendo GameCube, um, release coming out. I even went to one of the cube clubs in Boston when they had those back then, where you got to try out the game cube before it was released.

And anyway, I was really obsessed with that. So I started creating websites and like, No, there are terrible, like, you know, geo cities kind of websites with lens flares everywhere. Um, so that kinda got me into the HTML CSS part of it. And it kind of bridged me into the whole bulletin board system, uh, stuff that was, that was going on back then.

If people were around back then, then like PHP, BB. The bulletin, um, envision, um, yeah, I think, yeah, another bulletin board and then a whole bunch of other things, um, kinda got me into that and, uh, you know, figuring out how to set up my own bulletin board system. You know, you kind of had to start learning a little bit of code and how to host websites and that sort of thing.

I eventually just got started hacking the bulletin, um, was where I pushed me to learn PHP, which is my first programming language. Um, so I started adding like little tiny mods and changes to it and that sort of thing. Um, and then I eventually said, Hey, this, uh, people tend, doesn't work the way I want it to.

So I built my own bulletin board system and that, and that kind of, that kind of pattern, um, repeated itself over and over again throughout, throughout my open source career, or I would see something, um, and just kind of say, Hey, I could do this better, or maybe not better, but a different way in the way that I wanted to.

Um, so that was probably back in 2003, something like that. Um, and that's kinda, that's kinda what, uh, what started my love affair with code?

Beyang: That's awesome. And, uh, from there, what was kind of the path into delving into the world of like regular expressions and command line search tools? Uh, I assume it wasn't a direct path. It must have

Andrew: No, no. Um, yeah. Uh, I don't know if there's any, like, I think, I think the overall path, the very, the very brief version of it is, um, you know, I did the bulletin board stuff for a few years. Then I got into X 11 stuff, because again, the same thing kind of repeat itself where, you know, I had some window managers out there.

I three awesome open box X monad, I'd try kind of all those. And they just didn't quite work the way I wanted to at the time, this was probably back around 2010 or so. And, um, I wrote my own like little tiling manager that would sit on top of an existing window manager called pie tile. Um, and that kind of like got my feet wet with X.

Um, I created a, uh, fork of open box that supported multiple monitors that didn't quite work the way I wanted it to. So then I just started, that's also the same time that I scoped discovered go a little bit before the 1.0 release. So I'm like, Hey, I could, I could build a window manager in this language and to B be much nicer than using C um, I had tried Python and that failed.

Um, and, uh, yeah, so I kinda did that. And then, um, Eventually I found rust, um, several years later and I don't really know what caused me to get into  other than the fact that I was just kind of looking at it, like what libraries were needed and, you know, people were like, Hey, we need a re regatta syndrome written in rust and it should be like Ari too.

And I'm like, Hey, okay. So I started reading the Russ Cox articles on regular engines. And honestly, that's kind of what started my love affair with like, you know, lower level texts, primitives and searching and whatnot, um, was basically at that point in time before that, before Rostin, before I had read Russ Cox's articles, I don't think I had any particular love affair with rec exes.

Although I did do a freshmen project where I implemented a very basic reg X engine in assembly, which was fun. Um, it was several years before that, but, uh, I think it was mostly just a matter of, uh, serendipitous. I was, uh, I liked, I was getting into Russ at that point, looking for things to do and, you know, the rednecks engine idea just really kind of leapt out at me as something that seemed really interesting.

So.

Beyang: Yeah, nice. I guess one thing just leads into another.

Andrew: Yeah. And I guess just to finish off the rest of your question there, I realized that I didn't quite get to this command line search tool aspect of it, but rep rep was just, um, uh, came a few years after the rug X engine and its original reason for being was just to benchmark the ragexe engine itself in particular.

I, I think the reason why I initially did it was because. I wanted to see whether the rednecks engine and its overhead could be competitive with prep. So like, if you wanted to go implement a graph, I didn't necessarily want to, but if you wanted to, what would the reg X engine be fast enough, um, for that task?

And so like kind of, you know, built up some of the internals and I'm like, yeah, actually this is pretty much just fast as graph in most cases. So, um, you know, I just kinda took that and the silver searcher idea and ran with it.

Beyang: Yeah.

Andrew: rip rap is like nothing revolutionary, right? It's the same pattern of, Hey, I can do a little bit better or, you know, my version of it, that kind of thing.

Beyang: Interesting. Interesting. I want to dive into rope grab, but real quick, since you brought up the topic of window management and also specifically  three. So we had the creative  Michael staple, Berg on the show, a couple episodes back, and I just want to hear really quick, you know what? We're kind of. Uh, your complaints with existing window managers and you know, what, what were the features that you just had to have that, uh, made you implement your own?

Andrew: Yeah. Um, we could probably do a whole podcast on that alone, but basically what it came down to was I wanted to run three monitors, um, simultaneously and in a way that, you know, that worked well and. Um, pretty much all, I don't want to say all cause maybe there's one I missed, but as far as I could tell at the time, um, all window managers basically implemented, what's called the EWM H spec and the ICC em spec.

Um, and basically those are conventions for how a window managers and other windows, other clients window manager is itself, a client that's kind of a special client and the X server, how they communicate with each other. And it's like all these things about, you know, it's how you're able to have like a pager, um, uh, on a desktop that is implemented completely separate from the actual desktop window manager itself.

Cause there's all these different conventions that allow you to communicate, okay, what's the current workspace and what's the active one. No, and all that sort of stuff. So, um, those specs kind of won't go into it well detailed, but basically they hard-code the assumption that every single. Um, re window, there's exactly one workspace active for that route window.

Any point in time. And if you put aside the old school, X screen model of multiple monitors, where you can move your mouse between, but not your windows. Um, the modern day version of that is Exxon aroma, which was replaced with the resize and rotate or the RNR. Um, extension and those basically implement multiple monitors by stretching the root window across all of the monitors.

Um, so that assumption of one workspace per route window holds in all of these window managers, because the EWM H spec basically requires it. So if you have that, you, all of a sudden, if you switch your workspace, it switches the workspace across all the monitors. Not, not a big deal. If you have one monitor, maybe you can kind of get by with two, but when you have three, it's just like, Oh my God, I want to have one workspace on this monitor, one workspace on this monitor and so on.

And I want to be able to switch them independently and I want to be able to move one workspace to another monitor. And I just, I don't want to have any restrictions. So, um, you know, my fork of open box added that functionality. Um, and then eventually, you know, it just kind of became difficult to do some of the other things that I wanted and that kind of bled into, you know, Hey, it would be a lot of fun or so I thought to build my a window manager and that was kind of the driving reason.

Um, shortly thereafter, I believe Apple added that feature to maca West. Not that they stole it from me. Cause nobody, nobody knows about Wingo, which is the name of the window manager. And I don't know if I ended up adding it. I know ex Mona had it way before me. So, um, yeah, that was kind of what led to that, to that window manager.

Beyang: interesting. Around what year was this?

Andrew: Uh, I think window, the first like release that I could actually use was somewhere around 2012, maybe 2011. Somewhere around there.

Beyang: Got it. Yeah. I wonder what the percentage of programmers out there, uh, is, who are just kind of obsessed with like getting their winter manager to behave the way they want to do. Cause I've definitely gone down like rabbit holes,

Andrew: Yeah. Um, I think there are a lot of them. By and large, uh, most people probably get by with existing window managers, but I am certainly not the first and I won't be the last to have written a window manager because something didn't quite work the way I wanted it to, or I wanted to scratch an itch. Um, but yeah, I've been using that with no manager since I, since it was usable in 2012 or whenever it was.

Um, I even, I think my, probably my only other user is my wife and she runs it on her laptop. So, so far so good.

Beyang: that's great. Um, Cool. So rip grab. Um, we got a little bit into kind of the beginnings, uh, just now, but before we dive further, can you explain what it is and why, why does it matter? Someone who may not be as familiar with it?

Andrew: Yeah. Um, so rip grep is, you know, at its, at its simplest subscription is a tool that will search, um, a directory tree of files, mostly plain text files, um, for a rug X or literal word, um, and will report the results to you. Um, you know, the simplest model is if you can think about it, it's just to explain the algorithm, which is.

For every file that you want to search, you iterate over the lines of that file. If the pattern you provide matches that line, then you print the line. Right. Um, and, uh, you know, that kind of like simple algorithm, you know, is, is built on top of right. All of these extras features and, you know, Rick represent for us to do that.

Uh, canoe grip has tons of features. If you go and look at it, the main difference between rep rep and canoe rep is probably that it's. It's often it's default or optimize or code search. Um, and what that means is that, uh, we'll do recursive search by default. So if you don't specify a file path, all you do is type RG pattern.

It will just search the current directory. Whereas with crap, you have to pass a dash armor flag. And the other, the other big thing here is that it will respect your, get ignore files, your dot ignore files, your dot RG, ignore files. It will skip binary files and it will skip hidden files. So all three of those things are what's called smart filtering for some definition of smart.

Um, and basically what that does is it's is it does a search and it searches the files. You probably want to search in most cases, and it doesn't search the files that you probably don't want the search in most cases. Um, and I think that's kind of why it matters. Um, the defaults are from a user experience level or why it matters.

And, you know, I would say, I would say secondarily to that is, is performance and. Um, and in a large number of cases, rep rep in canoe, grapple perform similarly. But if you look at it from the user experience perspective, rip rep will often perform much faster than canoe grip because, because of the fact that it will do parallelism by default and, um, you know, that that's probably the primary reason, right?

If you, if you just run, rip, rap over a repository, it's going to skip some files. So if it skips some, several gigabyte files, that will be why it's faster for one. Um, and the second, you know, obviously it's like a thing of being that it uses parallelism by default. Um, and you know, I think that matters. So the default mode of operation for rip rep is that it has better results from what you want.

It, it has less noise and it tends to be faster, not because of any innate algorithm difference. Although once we get into more details, there are some algorithmic differences, but basically just using parallelism is, is, is the, is the main thing there

Beyang: Yeah. As a rip grip user myself, it's my main command line. Cool. I have to say that to everything you just said is, is a 100% true. It feels, and it is so much faster. I remember, you know, back when, uh, I used grip a lot, every kind of thing since of wanting to use graph was kind of an exercise in trying to remember which command line flags, uh, we're the right ones to pass.

Uh, you know, what directories did I need to exclude? Uh, and, and because of that, like, I probably didn't grip as much as I want to do just cause there was always this like, ah, like, ah, I gotta deal with that again.

Andrew: Yep. Absolutely. That was, that was my experience as well. I had, I had, I don't know how many wrapped different wrapper scripts around graph that would, you know, only search go files and you know, that sort of thing or whatever that would, that would basically, you know, it would approximate some of the defaults of rip rep and silver searcher.

And of course, act before that.

Beyang: Yeah. So you mentioned those other grapple alternatives, uh, silver searcher, um, ACC. Uh, so these predate, uh, rip grip, um, did you use any of those before creating rip grip?

Andrew: I did not actually, um, I, uh, I had known about it silver searcher and was vaguely aware of act at the time. Um, for whatever reason, I was just kind of like, it's kind of repeated a lot of things that you hear, um, people who don't use rep rep today, which was act reps, go to them and, you know, for the most part, yeah, that was true for a lot of things.

And I think it was just, I never really gave the silver searcher. Uh, try, um, uh, I think I, I used it before, certainly when I was building rep rep, I started using and trying silver searcher more to understand what the competition was like and what the existing, the existing tools were like. But before that point, I don't think I had ever used it in anger.

I just use my grip wrapper scripts just because it just worked well enough. And that's kind of like the story of tools, right. It works well enough. You don't really have a reason to use something new.

Beyang: Yeah. It's like, it is the expected gain I get from using this additional tool worth the added complexity of learning that new tool. And then, you know, God forbid I switched machines

Andrew: Yeah, absolutely. Yeah. It's another obstruction. It's another layer. I don't know if I'd say abstraction, but it's definitely another layer of, of stuff. And it's, it's not, it's not in, you know, core utils, it's not ubiquitous. Lot of people know about it and it's in a lot of repositories, but it's not going to be on every machine, like preface.

Beyang: Yeah, that makes sense. So, so you mentioned, you know, when you created rib, you didn't really go into it wanting to make a grip competitor, an alternative to grab you. You started with a regular expression engine, and then you wrote this thing as a way to kind of benchmark, uh, that engine. It turned out to be like performance, like competitive with, uh, all the other solutions. What, what was, um, I guess at what point did you say, like, okay, I'm going to actually invest in, in making this into like a really neat command line tool that, you know, I, myself am going to use day to day.

Andrew: That's a good question. Uh, I don't know if there was a specific cut-over point where I said that to myself. Um, I mean, I guess there has to have been, but I think, I think really, it was just a matter of, Hey, This is competitive in a lot of cases, you know, not just some of the cases that I had that I had expected that it might maybe, or maybe didn't even believe that it would be.

Um, because at the time, you know, I kind of had the same impression about canoe grip speed as everyone else. And I say that with there's two well-known articles that are, have gone around the internet, um, for a long time, one of them was the ridiculous fish. Um, post about, you know, the treacherous optimization and how, you know, um, he was looking at how to optimize the border more algorithm and that sort of thing.

And then there was, you know, the, the famous, um, uh, posts from, I believe that I don't know how to pronounce his name, like cartel. I believe the original author of good new grip. Um, and he posted to the BSD list around 2010 about why the new grep was fast and, you know, he listed all these different reasons and it was like, Oh my God, you know, how can you ever beat the new graph?

So, uh, I think, I think once I, once I actually had a tool and like a very basic tool, right. That was useful for benchmarking and was somehow competitive with canoe graph. I was like, Hey, you know, I could turn this into a real tool. I think the thing I was benchmarking at the time was, um, whether if you have a match on every single line, I was benchmarking, benchmarking that procedure and that procedure, a lot of the optimizations don't matter too much because.

You're having a match on every single line. So there's a ton of overhead, right? You, you might as well iterate over every line and try to run the reg X and that sort of thing. Um, and, and, you know, even, even a naive approach will perform similarly there. So the main thing that I was testing there was, does the overhead of the record extension is that competitive with good new ground.

And, you know, that turned out to be okay. I think that that caused me to do so optimizations and the rednecks engine, but then when I tried other cases where the matches are less frequent. And that's where the performance tricks really start to show the main difference between a naive grep and one that's, you know, been optimized crazily.

So, uh, and I think once I tested that case and that's, and that's when that became competitive, that's when I said, wait a second, I could go build a tool that does better than silver searcher.

Beyang: Got it. What, what portion of the kind of end user, uh, perceived performance gains come from? Just being smarter about what to search over versus, you know, the performance of the underlying rejects engine and algorithm itself

in your estimation.

Andrew: Yeah, that's a great question. Um, I think the first thing to tackle there is the smart filtering, um, because oftentimes that can actually make the search go slower. Um, even though it skips files and causes, uh, the tool to search fewer things, um, the actual process of resolving all of the globs and the get ignore files can cause the search itself overall search time to be slower because, um, Uh, it takes time to build those glow, those get ignore files into glob matchers, and it takes time to match those.

All right. So if you have a deep directory and every directory in that level has a different get ignore file in order to implement it correctly, you have to match a file path against every single get ignore file up to the root of the repository. Um, and then there's, you know, like global get ignores and sorts of other things.

And then there's the rip rep ignore files and those take time to process. Um, but if you have some, you know, huge binary files, maybe they're build artifacts, executable, files, images, whatever. Um, you know, I hesitate to say canoe, grapple source, all those, because it has some optimizations for not doing as much work in a binary file, but for the most part, rip rep will be a bit smarter about skipping those.

Um, it will also skip your docket folder, which can get quite large and big repositories. Whereas the new grep will not. Um, you'll often see like canoe grep wrapper script. So we'll have like a dash dash exclude dot yet. Um, and that will that on its own could speed up the search quite a bit. Um, so you kind of have that level of thing, obviously, if there are huge files that are being skipped.

That's that's, that's one aspect of the performance improvement. Um, I think the next thing and that comes from that, particularly when you're doing recursive search is mainly just parallelism because canoe grant does know parallelism. The only way to do parallelism with canoe graph is to use some other tools such as the new parallel or, um, ex harks.

And, um, you know, those implications are not too bad, but there. They're extra stuff that you have to do. Um, I know that there have been efforts to parallelize a new grant, or I believe there have been, and it's been done because canoe graph, you know, it was probably written in a day where parallelism wasn't really a big, it wasn't really a predominant paradigm.

Um, and so there was just, you know, global mutable variables everywhere. At least last time I looked there was so, um, you know, that's kind of the second thing that, that will, that will make regret faster in the vast majority in a large number of cases. Um, and I think the third thing is probably, you know, the rednecks engine itself and some of the literal optimum.

Mmm I'll describe one of them because I think it's the most interesting one. Um, And I'll, I'll do my best to make it brief, but basically, um, when canoe graph implements boy or more, um, if you're, if you're, if you, if you've done substring search algorithms implemented substring search algorithms before, um, or more the main thing about that algorithm that you're supposed to remember is that it will skip characters.

Um, sometimes it we'll look at, we'll look at a character and it will consult a mismatch table. And if that character doesn't appear in your search string somewhere, then you know, those, the algorithm will know to skip over X number of characters and move on to the next piece. And, um, you know, especially my cartels, um, post, uh, The BSD mailing list kind of popularize this thing.

Hey, we don't actually look at every bite. Um, it turns out that I'm a hundred hardware. Um, you know, this may not have been true back when probably wasn't true back when my cartel, but I've written a blog, uh, sorry that mailing was supposed, but nowadays on modern hardware, that's actually not the thing that makes more and more for fast.

What makes boy more fast? The Skift loop and the skip loop looks at takes the last bite of the pattern. And it feeds that to a routine called mentor, um, and mentor is provided by Lipsey and on most platforms that operation, which is take a single bite and look in some haystack of bites. Is super optimized.

Um, so it will have implementations using, um, like assembly implementations. Gillepsie does canoe Lipsy has assembly implementations for SSE too. And for AVX, you know, arm implement arm, the arm platform will have assembly and limitations for it. Um, you know, mutual has its own, um, version and see, so it doesn't, it doesn't use assembly, but it uses C um, and so on and so forth.

So if you have. A really rare blight that you feed to that mum char routine. It's going to stay in that Metro routine for the vast majority of the file that you're searching right now. If you happen to happen to have a very frequent bite. Just so happens to be the last bite and your needle, then that man shot routine is going to run.

Stop, run, stop, run, stop. And you just going to be constantly ping pong in and out of the mantra routine, which has, you know, it's just going to be slow because you want to spend as much time as you can inside that Cyndi vectorized loop. And so basically, I don't know of anyone who has done this optimization before, but.

You know, I had the insight that you just have a background, frequency distribution, a heuristic, you just estimate it. It's not, it's not something that changes. You know, you take some common texts, some source code, and you say, Hey, this bite is really rare and this bite is not so rare. Um, and instead of always picking the last bite, instead, you look through all the bikes in the needle and you look at the one that is probably the rarest and, uh, you feed that to memoir instead.

And that's your skip loop and that's where are they? The vast majority of the performance improvement, I think comes in the common cases, I should say, in the common cases, um, from, from rip grip, um, when you're searching just a single file when compared to the new grip, um, canoe grep could implement that optimization.

Um, I don't know if anyone's tried, um, But they could. So, uh, you know, I think those are the three things, um, for common cases is, um, skipping really huge files, sometimes parallelism and, you know, heuristic, uh, uh, heuristic, uh, tweaks to existing sub string algorithms, um, based off of, uh, background frequency distribution.

Beyang: Got it. Is it that optimization that you described, uh, took advantage of, I think very domain specific knowledge about, uh, the, the mem char routine. Did you know, were you familiar with that going into this, or was this something you uncovered in kind of like looking through the, the critical regions of, of the code as you were doing performance optimization group grip.

Andrew: Um, that's a good question. I don't, I don't know. Actually, when I realized, I feel like knowledge of mentor is kind of always been there, my brain. Um, but I, I don't know. Um, I know Russ Cox has articles mentioned memoir. There's a, there's a brief call out to it. Um, but otherwise, um, I mean I've implemented mentor.

So if you use the mentor, crate and rust and the rusty ecosystem, then you'll use migrate and there's, um, a Russ, uh, implementation of it using APX. So, um, you know, when you use mint, when you use grip crap on Intel machines, you're using mine plantation and memoir, not gypsies. So the fact that I had also implemented it kind of also.

Gave you knowledge of, of it. Um, but yeah, I think it's kind of just one of those performance. It's one of those performance tricks that kind of has its own general rule, which is you have a fast path and then you have the slow path. And a lot of times the performance optimization, assuming the fast path already exists is really just about causing more inputs to use the fast path in more cases.

And, um, that's kind of the case here.

Beyang: Got it. So you, you've kind of drawn a contrast between rip grip and grep and explain the kind of performance differential between those two. Um, but what about the more recent grep alternatives?

Andrew: Mm. Hmm.

Beyang: rip grep is faster than, than those, but all those were written more recently than grap.

I think grep was, you know, made back in what, like 1970 something or other, so like, you know, presumably they had access to the, you know, similar knowledge and, and similar awareness of modern systems as you did. What do you think contributed to. Ripped rep's ability to outperform those more recent alternatives.

Andrew: Um, Oh boy, that is a complex question. Um, so just the first Claire, just, I want to make a first clarification. Uh, grep was invented, I believe back in the early seventies and it was invented by Ken Thompson. Um, and as far as I know, I don't know. No, if there's any direct descendant of that specific program that he wrote, that's still in use.

Um, yeah, maybe, maybe a plan nine system maybe, or one of the BSDS might use a direct descendant of Ken Thompson's original grip, but I don't know. I actually don't know. Um, but you up, I believe was written in the early nineties or late eighties. Um, and that was by the hotel. Um, and. Yeah. I don't know what that was before vector instructions.

I'm pretty sure vector instructions existed by then, but I don't think they existed on Intel platforms as far as I know. Um, but, uh, Yeah. So, so let's, so just getting to the meat of your question, which is grep alternatives and why aren't they as fast? Well, um, so I'll just talk about to act and soul searcher.

I think they're the most, the most popular people know about there are, there are several others that I know about. Um, but I'll just talk about those two. So act is written in Pearl and it uses Pearl's ragexe engine. Um, as far as I know, ACC has no. See code in it other than, you know, Pearl's implementation of the reg engine, which I believe it was just I'm sure as in C um, I know it wasn't CF.

I've read it before. So, um, there's, there's like that aspect of it and. Um, as far as I know, Axe author has not prioritized performance. Um, his main thing is, um, the UX, the user experience of it, the skipping of the files and the features that are INAC. So as far as it being slower, I was, you know, I don't think I've ever done any, any in depth, performance research there, but, you know, waving my hands.

I'd say, well, You've written, it it's written in a higher level language. And even if the right X engine is in C, you still have all this other glue code that's in the higher level language. And it has to communicate back and forth between C and I would say that's probably where, where the performance difference comes from.

Um, as far as like pearls, rednecks engine, I've never done any kind of performance comparison between it directly and my own, just because it's hard to do, because as far as I know, there are no C bindings, the pearls Excedrin. So. Um, there's that? So silver searcher is, uh, I have a big FAQ entry on it in my, in my rip rep repo.

So for people that are more interested in, in the details, they can go to that. But I would say that the first thing is that memory maps on Linux. If you're using them. On a bunch of tiny files and a repository is actually worse than not then not using memory maps. Um, silver searchers, Remi claims memory.

And that's the reason why it's fast. And as far as I can tell memory maps only make a difference when you're searching huge files and there's a small performance improvement. Um, so that's the first thing. The second thing is that it, and the second thing is that it's parallelism is only at the search level.

Whereas rep reps parallelism is both at the search level and the directory traversal level. And that's important for a really critical reason. Um, sir, there's the performance gains that you get from doing directory traversal using parallelism on its own? Um, they're small, but they're there, but the big one is the get ignore processing.

Um, because is it good? Ignore processing is built into the directory traversal because if you are ignoring a directory, you don't want it to send down to that directory. So if you paralyze directory, traversal, then you paralyze get ignore matching. And a silver searcher does not do that. Um, but Roker up does so that's, that's another reason.

Um, I don't know how much of a reason it is, but it's one and say, I'd say the next biggest reason honestly, is, is some combination of the rednecks engine and literal optimizations. So silver searcher does like the most basic literal optimization that there is, which is if the pattern is a total, uh, the whole entire pattern is just a literal.

Then I think it, it defaults to its own, uh, Boyer, more implementation. Rick rep's literal optimizations are much more sophisticated, um, on the level of canoe greps. Um, and basically what it does is if you have a literal in the beginning of a reg ex a literal at the end of a rug X, or even some combination of literals in the middle of a rug, X rep rep will detect those.

And it will use a sub string search implementation on those, and it will search for those. And if it finds a line that matches, then it will run the reg X engine on that line. And that's kind of the same reason why canoe grep is fast. Cause it does the same kind of optimization. So I'd say that that combination of things is what makes it faster than the grip alternatives.

Beyang: Got it. You know, you have such a good command of, uh, what the bottlenecks and like critical regions are in both your own code and the code of, uh, tools that do similar things. Do you have like a, a process or a standard toolkit that you apply to identify and, and, um, you know, build up this understanding.

Andrew: Uh, I think, I think the short answer is just science. Um, it's, you know, it's, it's the scientific method, right? You, you form a guess about something based on your experience, um, and you go and test it and you try to prove yourself wrong. And. Um, if you're right, then you should be able to find evidence that supports, uh, your guests and, you know, events.

Actually, you do that enough, you develop experience, you develop kind of like an internal model of, of what, how, what things are doing and how they behave and where the bottlenecks are. Um, and you know, that kind of is what gave me my knowledge, I guess I would say, um, reading other people's experiences, their blog posts, the information that they've shared, their code comments in the code that says, Hey, this is a performance bottleneck.

Okay. That's. Maybe, maybe it's not correct, but it's, it's something you put in your brain as, okay. I can test that later. Or if there's something that I run into with my code, maybe that's something I can look right. Because when something is slow and you don't know why, which happens still happens to me.

And you don't know why you kind of have to use your tools, your profiling tools, perf Val grins, looking at the generated assembly. You know, using your own experience to guess at where things might be slow or fast, even Prince LN profiling, right? Printing out timestamps at certain points. It's all those sorts of tools are available to you.

And, um, you know, doing those things over and over again. Um, and when you don't know where hormones bottleneck is, you kind of have to just narrow it down with those tools. Um, and when you do narrow it down, you've added a data point to your experience. Hey, I had this problem and the problem ended, bottleneck ended up being here.

And if you do that enough, you get some experience, you get some intuition. Um, but, uh, it's when it comes to performance, um, you always want to check your assumptions because even when you have tons of experience doing it, um, you can still, and, uh, Um, yeah, I think that's, that's probably a good enough answer for it unless you have other questions.

Beyang: Yeah, no, I think my reaction to that is like with regards to science, the actual scientific method part is, is kind of the easy part. It's the, uh, hypothesis,

Andrew: Yeah, that's true.

Beyang: part that is really the secret sauce. And,

Andrew: Yeah. That, that is, that is a really good point. Um, you're absolutely right. And it took a long time to bootstrap that. Right. So. You know, how do you, how do you even feel like, is it a little bunch of unknown unknowns? Like things you don't know that you don't even know? So, um, you know, how you, how do you deal with, with that sort of situation and, um, you know, asking for help or taking, learning the tools, if you, if you're not, if you're not, if you're not sure where to start learning the tools and, and as best as you can.

And, um, Just trying to narrow down where the problem is. Where's most of the, where's the code spending most of the time. And you know, if you're having trouble reading some assembly, because oftentimes it's when it comes down to you're using R you're profiling tool and you're looking at the generated code, and I'm not sure how to read this, you know, hop on a discussion forum or a channel chat channel somewhere.

And I'm sure there's someone who'd be happy to help you decipher some assembly. Right? I mean, there are people who love doing it. So, um, Yeah.

Beyang: Cool rip grip is now just insanely popular. Like a, I think everyone on the engineering team at Sportscraft that I know uses it. Um, and kind of from the, like how, how old is the project now? Is it a couple, a couple of years? 

Andrew: As of September.

Beyang: Was there a point at which it really started to take off or was it kind of this steady , you know, growth  as I went along, do you, do you recall, or

Andrew: Um, I think the only, other than like, get on stars, which, you know, people have opinions about whether they're meaningful in any, in any, it it's, it's a proxy. It's a proxy of something. I don't, I don't. Yeah. So there's that. And as far as I could tell that it was a steady growth over time. The only other thing that I've really monitored.

There's two other things want hear. One of them was, um, I will occasionally look at the home statistics, so they have kind of like that. I forget whether I know there's a controversy, but a while back about auto opt in, but they collect analytics on how many times the packages installed. So, you know, Rick rep is on that list.

Um, I think it passed the silver searcher a while back. If I remember correctly, that was like one milestone. Other than that, the other, the only other thing that, um, I really am aware of is, um, Rick rep's adoption into visual studio. Um, that happened actually, I can't quite remember exactly when that happened, but I want to say it's at least 2018 or earlier.

Um, but yeah, just simply remember that. Definitely created an uptick in awareness about rep rep, um, uh, you know, cause it was in its release notes and wants people use vs code and that sort of thing. I know people see it in their processes tab on windows. I've seen that screen. What is this RG exe process using up all my CPU.

Yeah. So I kind of, so I've kind of seen that. Um, yeah, otherwise I would, I would personally say steady. There was kind of like a big bang when it was initially released. Um, and other than that, as far as I can tell a steady, but I don't have any kind of like monitoring tools where I can actually tell how popular it is.

Beyang: Got it. Now that you have so many people using it, um, to what extent do you allow, you know, external feedback to drive your future plans and roadmap versus how much do you still lean on your own intuition and your own kind of personal needs?

Andrew: Yeah. Um, I've actually, I can think of off the top of my head, two big things that I've reversed coursed on reverse course on, um, uh, in the, in the past, um, which was a multiline search and, uh, uh, support for. For look around and back references, um, which inevitably was added via adding a new Rex engine.

Cause I didn't want to add that to the rest of our extension. So I added a PC support for PCRE to both of them. Those things were things that I initially set out as, no, I don't, I want to do them because they make the implementation too complex and you know, The implementation is too complex. It becomes harder to maintain.

And I'm, I'm trying to build a, trying to, I want the project to be sustainable where I'm happy to maintain it because it's not too much work. So you know, that, that, um, those kinds of drive that kind of decision your process is primarily what drives my inclination for adding new features at this point, um, is, you know, if this is going to turn into a project, I don't want to maintain, like, for instance, Wingo.

Um, that window manager is basically, you know, it's, it's in maintenance mode and it, I keep, I keep the lights on. But I don't do any new feature development and I don't, I don't really want Rick reps to get to that place. I want to continue to evolve and to continue to improve on performance. History says that is not never going to be the case.

There's always going to be some new young tool that comes out. And does something better or smarter it exploits some new hardware feature better, it exploits some new interaction. Maybe there's a new source control program source control thing that we move on to from get. And the way it does, I don't know, I'm just we're spitballing here, but the way it does it ignores files.

There's some new way of traversing files that allows, um, a search tool will be faster.

So in terms of like future direction and like how other people influence it. Um, I try to let you know if there's a lot of people requesting a feature. Um, then I tried to let that guide things as long as we can find a sustainable way for my, for my definition of sustainable, basically, um, to implement it.

And if we can then, um, you know, I'm willing to accept new features that 

Beyang: Cool sounds very reasonable. Um, you mentioned that, uh, you, you added support for PCRE, uh, you know, new, new, regular expression, uh, engine, uh, to rip prep. Uh, I was wondering if you could talk about. Um, kind of the varieties and different approaches that you've seen to regular expression matching in the wild.

Um, because from my point of view, you know, regular expressions, there's something I learned about in, you know, a four year computer science degree. Uh, it was presented in a way that suggested that, you know, these things have been around for awhile and there's kind of a standard way to, um, you know, create a matcher.

And then, um, in. Yeah, the actual real world I've come into contact with a variety of different regular expression engines and have been surprised at like the kind of diversity of  styles and performance, trade offs and things like that. So, um, you know, what, what are the different varieties of regular spectrum, a person that you've seen and why isn't it as simple as like, Hey, you know, this thing translate it to a DFA and, you know, run the DFA and then you're done.

Andrew: Yeah, that's, that's another question that we could spend a whole podcast on, but, um, I think if we limit ourselves to a general purpose, reg X engines, Um, where, you know, it's like a reg X library that you might use, like what's the rec center. Are you using PHB? What's the reference engine you use when you're writing Perl and so on and so forth?

Um, we limit ourselves to that kind of scope and we put aside like the specialty rug extensions, where you might have something like Ari Tusi or, Oh God, I'm blanking on the name, the popular one from. Wow. Okay. I'm blanking on the name, but it's, it's a one where you're able to write and generate DFAS ahead of time.

Um, and you can, you can execute certain actions for them, but if you put aside those things and you basically limit yourself to general purpose robotics engines, you kind of have to kind of constraints put on you that I can think of off the top of my head, which is, you know, completion time has to be reasonable.

And, um, mash time has to be fast, ideally fast. Um, and you know, if you're at that level, there's really two different ways is to implement a reg X engine. One of them is with backtracking and that's kind of what PCRE does, what Pearl does, but Ruby's rung extension does JavaScript's Oregon does. And then there's, um, you know, the finite automaton.

Approach, which is usually something combination of, um, UN simulation non-deterministic finite automaton simulation, and the deterministic finite automaton simulation. I've never implemented. Um, Well, I've done backtracking before. Um, but I've never implemented like a, PCRE like backtracking reckless engine.

So I can't talk too much about them, but my general understanding of them is that they tend to break a reg X down into one of many different types of opcodes and the leg extension itself essentially executes like a VM. And the main feature of backtracking engines is that they have tons of features in them.

So like back references and look around and PCRE has a whole bunch of stuff like recursion and call outs and all sorts of crazy things, you know, really fun. Um, and, uh, the main, the main drawback of all those things is that it's hard to implement those things, um, in a way that's fast, um, from a, from a time complexity point of view, Um, so it's very easy to create a rec ex it'll take exponential time on the input.

Um, and basically that causes a class of security vulnerabilities called break ex denial services. Service attacks are E dos. And like this has happened in some high profile cases. I think CloudFare flare has happened to itself. And stack overflow is another popular one that I'm aware of in the last few years, but, um, That's kind of that side of side of it, robotics engines.

The other side of robotics engines is the one that I know of a lot better, which is the finite automaton approach to it. And I'll try to be really brief here, but basically, you know, if you implement, you know, the standard NFA simulation that you might see in the textbook, you'll get a great time complexity bound, which is, you know, um, the length of the reg X times, the length of the input.

And that's, you know, the optimal case. Um, and you know, the problem with that approach is that it's dog slow in practice. Um, and, uh, this actually happens to be what, uh, goes right to engine implements. Um, and it's why there are, there's an open issue on the go Trek issue tracker that, you know, says, Hey, make the record says go faster.

Um, and the, the, the, the typical way to speed that up. Um, again, hand-wavy here being very hand-wavy here. Um, the typical way to speed that up is something called a lazy DFA or a hybrid NFAT. And again, I'll be super brief here, but the primary difference between DFA is that an NFA can be in multiple States at once.

Whereas a DFA can only be in one state at once. And the main difference, the main implementation, the difference between those is that when you see a new bite, when an NFA implementation, you have, I have to go through every single state that you're in and process. Okay. This state goes to this state. This state goes to this date for, you know, for every state that you're in for that single bite.

And that takes a lot of work, especially with alternations or repetitions or whatever. Um, But, uh, with a DFA, you have a byte, you have a single state and you move from that bite and that state to the next day. And that can be implemented very efficiently with a finite number of instructions, a buck, sorry, a constant number of instructions and, uh, using, uh, a table in memory.

And basically a DFA and that implementation is going to be about more magnitude, faster than NFA. The problem with the DFA of course, is that, uh, building a DFA, uh, can have a potentially exponential number of States and the size of the rednecks. And especially when you have Unicode support enabled those cases are not rare.

They're very common. Um, and, uh, it creates a situation where compile time is just too expensive in a general purpose, right. Extension where you need to be able to compile a reg X may be in web request response time and be able to, you know, run that reg X. So, um, you know, the hybrid lazy DFA, the hybrid NFA DFA, or also called the lazy DFA is I believe something that Ken Thompson created or came up with.

And it's something that is used in various places, canoe grep does it. Um, Russ Cox kind of repopulate it by describing they get in his article series on, on Rolex engines. And that's how  was fast. Um, and, uh, that's, that's the fundamental reason why it's fast is because it combines NFA with DFA. It does not compile the full DFA ahead of time.

It only compiles what it needs for the next bite of input. And then the crucial bit is that a cash is the state that it created. So it sees the next bite. If that state had already been cashed, then it will, it will act like a normal DFA. And, um, it just repeats that eventually you get to a point where you've cashed all the States that you're probably going to see, which has maybe a very small portion of full DFA, and that tends to work great in practice.

Um, it's failure mode is that if, uh, you have to regenerate the state on every bite, um, and there are definitely some recces where that occurs then, um, uh, you know, you end up spending a lot of time recreating the CFA and it ends up being a similar speed as the NFL. But, um, thankfully those cases are somewhat rare.

So ends up being a decent trade off in practice.

Beyang: So it's kind of a adjust in time compilation approach where you're compiling the regular expression, or really not the regular expressions, like the NFA into the DFA, like expanding the state space of the NFA into DFA States,

Andrew: Yeah, you're, you're actually doing subset, you know, sub sub construction, which is the S which is the way standard way that you translate an NFA into a DFA. You're actually doing that subset construction on the fly. You're just doing it one bite at a time. And I would, I would definitely not use the word just-in-time or jet to describe it because.

That is actually a whole different implementation strategy and rednecks engines. Like, you know, the, the, you know, Google's VA JavaScript engine is, is, is, uh, has a jet to it. Um,  has a jet and both of those are impressively fast, uh, uh, engineering, you know, it's just amazing engineering efforts there.

Beyang: Got it. Got it. And the, in those jets are actually like compiling the actual regular expression into, um, some sort of machine code.

Andrew: Yup. Yup. They're there at, at, at, um, at runtime they're producing, um, assembly code that matched the rig and they're both of those engines are backtracking.

Beyang: Got it. Fascinating. Um, I hope we didn't lose too many people in that discussion, but 

Andrew: Yeah. Yeah. It was quite, not as brief as I wanted looking at the clock, but, um, it's, it's hard to succinctly answer that question.

Beyang: Yeah, totally. I think you did a fantastic job and uh, now I have some additional blog posts I need to go read.

Um,

Andrew: Yeah. Russ Cox is Russ respiratory. What's Cox, his articles on reg Xs. It just Google Russ Cox. Right. And it'll be the first thing that's there. If you haven't read those, or if anyone that's listening that hasn't read those, those are just, you know, pure excellence. There's just, they're just treats to read.

Um, and really just cannot, cannot recommend them more.

Beyang: Cool. We'll link to those in the show notes. Um, so I wanted to get to some of your other kind of contributions in the rest, uh, open source community. Um, you mentioned that rust, uh, I think it's safe to say that like rust is probably your go to language, uh, at this point. And you've contributed a lot of, uh, crates to, uh, open source community.

The, the rednecks crate, uh, being one of them. Can you talk about, you know, why you got into rust? What you like about it? Um, and, and how you see that, uh, community evolving over time.

Andrew: Yeah. Um, yeah, I would say it's fair to say that, um, at least in the coding that I do in my free time, my open source work Russ is definitely the language that I, that I use the most these days. Um, and for work I use go. Um, but as for us, you know, what attracted me to it? I think, um, you know, at the time I was big into Haskell, I was big into go.

And, um, I had, I had, uh, been coming off of a few years, spent in grad school, um, doing, you know, I was actually, my concentration was in computational biology, but. I had spent a lot of time studying programming languages and being a teacher teacher assistant for the programming languages course there, um, under Norman Ramsey and, and, uh, you know, I just got a love of programming languages and type systems from my time there.

And when Russ kind of came to the scene, I kind of been watching it for, for a while, but. For whatever reason, for some reason, somewhere around like late 2013, early 2014. And it hit a point where I felt like, Hey, I could learn this, the docs, there are docs there. You know, I could, I could start writing code.

So I, I wrote a quick check implementation, which. I had known from my high school days. And, um, you know, the reason why I wanted to write it was because I had seen all these similarity. These are intentional similarities, but seeing all the similarities between the trait system in rust and Haskell's type classes, and if you've ever used quick check and Haskell, it makes heavy use of, um, it's type classes.

And. Return value polymorphism and all this cool kind of stuff. And, you know, I wanted to say, well, our frustrates are they expressive enough to do this kind of quick check thing? And, and I was like, okay. So I did it. And it worked. And I was like, Hey, this is, this is cool. And so like, you know, I kinda like transferred.

Like I had, you know, I have a little transfer of knowledge and say, Hey, I knew all this stuff. I'm at school. And it works over here and rust. And it kind of the thing with Russ is that it kind of took my. My lack of systems, the stuff which I had gotten from sea and go, and my like of, you know, PL program language stuff that I got from Haskell nice type system features.

And it just kind of like crashed those together and I really, really loved and still love the result.

 Beyang: C and go versus Haskell very different, I think, uh, , language paradigms.

Uh, and, and you're saying that  rust is a synthesis of. Of those two. Can you, can you talk about, cause like, you know, normally when I've ever has like a stereotype of like, you know, your classic C developer versus, you know, your classic Haskell developer, they're very different.

So you talk about like why you seem to like both and what you think the D the two communities could learn from one another.

Andrew: Yeah. Oh my goodness. Uh, so I would say that like, you know, the whole Sego meets Haskell thing was kind of like my impression at the time, I would say a more, a better or more accurate, um, comparison would be, let's say C plus plus me task goal meets standard ML maybe is what rust is. I don't know. But, um, but in any case, uh, you know, . What I liked about seeing go or the things I do like about seeing go or that I really loved working at a lower level of abstraction. Um, I liked performance. I like making things go fast and, um, I found it difficult to do that with, with Haskell. Um, one of our research projects actually was implementing the Viterbi algorithm.

Um, goodness has been a long time, but it's a way of, of, uh, dealing with hidden Markov models and basically how to trace a path to a hidden Mark off model, to come up with a probability and the Viterbi algorithm, um, you know, deals with,

as in order to speed it up, you can do this a memorization technique. And I remember us, you know, one of our research, um, experience reports was, you know, how can you. Um, can you write the turbo? The implementation in Haskell that's is as fast as CS and we actually couldn't. We had an implementation of it and C standard ML and Haskell.

And, um, you know, we tried to apply strict in the sanitations and all sorts of stuff like that and Haskell, and you know, my experience there is, is really not unique as far as I can tell. If you scour the internet for blog posts about optimizing ASCO code, you kind of see that same problem over and over again.

Um, and you know, that was kinda my, my, uh, my path there as far as how those two things met and why I, why I liked it in terms of what the communities could learn from each other. I, I, I don't know, you know where to begin with that one. And I don't say that in a sense that I feel like they have a lot to learn from each other.

Like, I don't mean it that way. I think I just made it as it's such a big question. I don't, I don't know. Really, this is the answer for me on that moment.

Beyang: That's fair. Um, you, you also wrote this blog post, uh, you know, a lot of our conversations so far has been like deeply, deeply technical. Um, but I also wanted to get to, uh, some of your thoughts on kind of community building and evolution. Um, seeing as you've been a part of the rust community for awhile now, and, you know, as that language grows and gains, uh, broader adoption, um, there's more people entering into the community.

, and you wrote a blog post about, you know, how the community , should or would or could evolve, um, as it grows, um, care to share your , thoughts on that.

Andrew: Um, yeah, so, uh, I guess just a quick background, um, I've been a moderator for the rust community, um, since the moderation team, um, since its inception, which is, I think was around 1.0 release or a little before, somewhere around there. And, you know, basically our charge is to uphold the code of conduct. Um, you know, which is, which is basically a way of keeping a healthy community.

And, um, no, I don't, I don't, I don't want to like dis try to distill the code of conduct down to a single thing, but a lot of it has to do with civility and, and, and making sure that. You know, even if people don't like each other or they don't like how other people, what they do in their free time, you know, to some reasonable extent, um, you know, everyone has to treat each other with respect and, and, and it's not necessarily just about treating others with respect, but it's actually about, you know, moving forward and, and, and having, being able to drive discussions forward with, um, you know, a mutual.

Desire to improve the language, improve the ecosystem, improve the libraries or whatever, whatever it is that's trying to be done. Um, which is a longer way of saying, you know, be constructive and, uh, you know, doing that, doing that as hard, especially as the community grows. Um, as far as my blog post goes, um, I called my false story.

Um, I think my goal with that post was probably less about. Um, you know, my specific, basic moderation experience with rust about, um, you know, trying to improve the empathy that we have for each other, or maybe prefer maybe, maybe help others do perspective taking, um, because you know, a lot of us play multiple roles.

Like sometimes I'm the end user of software and sometimes on the maintainer of software. And, you know, those, aren't the only two roles in open source, right? There's no documentation writing event organizing community organizing, you know, all sorts stuff is a UI design. Um, and you know, being able to, to make sure you're taking the perspective of, of each person that you're talking to, um, is really my goal with that blog post.

But, um, I don't, I don't, I hope I succeeded with that, at least in some part

Beyang:  hard part of software, I think often comes at the intersection of like deeply technical discussions, as well as, you know, human and social side of things. And I think both are important to get, right. Especially since there's a lot of smart people out there and want to, you know, source ideas from, from, uh,

Andrew: especially when you're, when you have. You know, a vibrant and diverse community, like Russ, where, and, and, and in particular online community, um, you don't have the same sorts of pressures or social expectations that you would in, let's say an office environment, or even a remote office environment with a, with a bunch of coworkers, right?

Like that's a professional environment and there's a whole bunch of different pressures that influence your behavior. There. But in the online world where literally anyone okay. And comment, um, there's just, you know, those, those sorts of, of procedures of decorum and, and, you know, good faith assumptions and, you know, working together and that, that sort of thing, developing rapport with people that you work with day in and day out.

No. I mean, obviously that happens to an extent, right? Because there are contributors that come back and do things over and over again. And people are in the teams interact with each other, but by and large, there's always new people coming into the community. And it's very difficult to, to, to, to, um, to moderate those discussions because you don't have those, those base assumptions that you normally do in a professional environment.

Beyang: yeah. Makes total sense. Um, alright. This is going to be a complete non-sequitur, but uh, I wanted to get to this cause it's, it's such an interesting, uh, story, I guess. So, in addition to open source software and programming, another one, one of your interests is football or American football.

Uh, and when we were talking earlier, you told me this anecdote, um, involving like a software engineer that you knew and, uh, bill Belicheck. And I was wondering if you could, uh, relate that story to our audience. Cause I thought it was, it was just such a funny, um,

Andrew: Yeah. Um, I guess there's not too much to it, but, but, um, I actually have this story second hand or third hand through, through, um, through a friend at Tufts university. And, uh, basically what the story is is that, uh, bill Bellacheck, this was some years ago, several years ago. Um, he was looking for someone on that, um, you know, to help him what he was looking for up for help with, but something to do with tech, something to do with computers or programming, whether it was, you know, analyzing injury statistics, or I don't know who knows.

Beyang: so sort of like data processing

Andrew: Some sort of data processing, looking at like tendencies of other teams, just to get a, just to get a tiny, slightest bit edge over the team, whatever, whether it was data scientists or mundane programming tasks. I have no idea, but from what I've heard, he, what he wanted was someone who understood tech and understood football.

So he basically looked at the intersection of those two fields in the area. And from the story that I heard, he found like a few people, three people. And one of the, one of the, um, people he found was someone who played football at Tufts. That also I was majoring in computer science and with someone that I happened to TA and, you know, that's, that was kind of.

That's that's pretty much the story was that I don't know what ended up happening or if he's still there or if he did anything cool. But whatever he did, I'm sure I'll never going to find out. Cause they probably keep that stuff very, very secretive, but

Beyang: Yeah, bell check recruiting both the best athletes and the best, uh, programming minds out of the nation's leading university. Yeah. I feel like it just illustrates, like for anyone who claims that software isn't eating the world, you know, even, even, uh, you know, coaches and in professional sports are hiring computer scientists to give them whatever edge, uh,

they, they can grab over their competition.

Um,

Andrew: myself have had a couple of NBA teams reach out to me to recruit me and you know, so it's yeah, it's there.

Beyang: if, you know, uh, you know, us, us programmers, uh, will ever reach kind of the celebrity status or, or notoriety as, as some of the athletes do. Maybe, maybe in the future, maybe it's a pipe dream, but maybe in the future, you know, ESPN commentators will talk about like, Oh, you know, the LA Lakers, they have such a good.

Uh, um, algorithmic basketball team.

Andrew: Yeah, I suspect not, but, um, but, um, I, I think, you know, maybe there exists a reality where that happens and maybe that reality is when like, uh, Bigger proportion of the population writes code. I don't know that that will exist. I'm not even saying that I want it to exist because I know that all those things are controversial, but maybe that reality does exist in our future.

And if there are a lot more people writing code, then all of a sudden that becomes, you know, something that a lot more people care about.

Um, I don't know, but other than that, I doubt it.

Beyang: Alright. Uh, on that note, um, my guest today has been Andrew Gaulin. Andrew. Thanks so much for, uh, coming on the show.

Andrew: My pleasure. Thanks so much for having me.
<!-- END TRANSCRIPT -->
