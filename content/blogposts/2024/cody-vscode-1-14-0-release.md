---
title: "Cody for VS Code v1.14.0: Now with bigger context windows and a refreshed chat UI"
authors:
  - name: Alex Isken
    url: https://handbook.sourcegraph.com/team/#alex-isken
  - name: Justin Dorfman
    url: https://handbook.sourcegraph.com/team/#justin-dorfman
publishDate: 2024-04-17T10:00-01:00
description: "Cody for VS Code v1.14.0 is now available. This update includes expanded input and output context windows, a new chat interface, improved unit tests, and more."
tags: [blog]
slug: "cody-vscode-1-14-0-release"
published: true
heroImage: https://storage.googleapis.com/sourcegraph-assets/blog/cody-vscode-1-14-release/cody-vscode-1.14.0-og-image.png
socialImage: https://storage.googleapis.com/sourcegraph-assets/blog/cody-vscode-1-14-release/cody-vscode-1.14.0-og-image.png
--- 

[Cody for VS Code](https://marketplace.visualstudio.com/items?itemName=sourcegraph.cody-ai) v1.14.0 is now available! This update includes context window expansions, a new chat interface, easier ways to add context to chat, new models, and more.

## Bigger and better context windows to improve chat

We've historically limited Cody to a maximum context window of 7,000 tokens, meaning that Cody won't pass more than 7,000 tokens (small chunks of text) to the underlying LLMs. This is mostly to make sure that _too much_ context isn't passed into the LLM, which can lead to poor context recall and then poor answer quality. 

Because of Claude 3's [improved context recall](https://www.google.com/url?q=https://www.anthropic.com/news/claude-3-family%23:~:text%3DLong%2520context%2520and%2520near%252Dperfect%2520recall&sa=D&source=docs&ust=1713389874874139&usg=AOvVaw2xo3VN2mbBXrMP3p9cCkLc), we can now expand that context window without degrading chat quality. We've made huge updates to Cody's maximum context window for Claude 3 Sonnet and Opus models:

* 30,000 tokens of user-defined context (user @-mentioned files)
* 15,000 tokens of continuous context (user messages and context that's sent back to the LLM to help it recall earlier parts of a continuous conversation)

_Note: One line of code converts to roughly 7.5 tokens, so these limits are approximately 4,000 and 2,000 lines of code, respectively._

This update means two things:

* You can now push _way more context_ into Cody, including multiple large files, so you can ask questions about larger amounts of code
* You can have _much_ longer back-and-forth chats with Cody before it starts to forget context from earlier in the conversation

We've also increased the _output_ token limit for all messages generated by Cody's underlying LLMs. Outputs were previously limited to 1,000 tokens, and we've quadrupled it to 4,000 tokens. This means you shouldn't see Cody's responses getting cut off mid-answer anymore. This output limit update applies to all models.

For now, these changes to context windows apply to Free and Pro-tier users, with changes for Enterprise users coming in the future.

## Revamped chat interface

The chat interface is getting a new look! Cody's chat log is now structured as a series of cells, each showing a message alongside the sender's avatar. When Sourcegraph fetches context to use in a request, Cody shows the context in a cell alongside the Sourcegraph avatar.

<Figure
  src="https://storage.googleapis.com/sourcegraph-assets/blog/cody-vscode-1-14-release/new-chat-ui.png"
  alt="Cody's new chat interface design"
/>

## Unit test command improvements for JS, TS, Go, and Python

We've improved Cody's unit test command for JavaScript, TypeScript, Go, and Python code in two ways:

1. Cody will now determine the correct range of code to apply the command to based on the code syntax
2. When you trigger a Quick Action (<kbd>Ctrl</kbd> + <kbd>.</kbd>/<kbd>Cmd</kbd> + <kbd>.</kbd>) on a function symbol, Cody will now given an option to generate a unit test

<Video 
  source={{
    mp4: 'blog/cody-vscode-1-14-release/unit-test-improvements'
  }}
  loop={true}
  title="Using the Cody unit test quick action"
/>

## Add context to chat using right-click

You can now grab code from the open file and use it as context in chat without having to manually `@-mention` it.

Highlight the code you want to use, then right-click -> ‚ÄúCody Chat: Add as context.‚Äù The highlighted file range will be added as an `@-mention` in the chat sidebar.

<Video 
  source={{
    mp4: 'blog/cody-vscode-1-14-release/right-click-shortcut-context'
  }}
  loop={true}
  title="Adding context to Cody chat using right-click"
/>

## New model support: GPT-4 Turbo and Mixtral 8x22B

Mistral just [released Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/), their latest open source model, and it's now available as a chat option for Cody Pro users. The model scores highly in math & coding benchmarks, particularly among open models.

We've also upgraded Cody's GPT-4 Turbo model from the preview version to the newer, non-preview version.

## Experimental support for CodeGemma + Ollama

You can now use [CodeGemma models via Ollama](https://ollama.com/library/codegemma) with Cody. The CodeGemma family supports fill-in-the-middle code completion and can be used to power Cody's autocomplete functionality.

## Changelog

See the [changelog](https://github.com/sourcegraph/cody/releases/tag/vscode-v1.14.0) and [GitHub releases](https://github.com/sourcegraph/cody/releases) for a complete list of changes.


## Thank you

Cody wouldn't be what it is without our amazing contributors üíñ A big thank you to everyone who contributed, filed issues, and sent us feedback.

As always, we value your feedback in our [support forum](https://community.sourcegraph.com/), [Discord](https://discord.com/servers/sourcegraph-969688426372825169) and [GitHub](https://github.com/sourcegraph/cody/discussions). Happy Codying!


---

**To get started with Cody [install it from the VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=sourcegraph.cody-ai)**
